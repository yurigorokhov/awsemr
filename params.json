{
  "name": "AWS EMR",
  "tagline": "",
  "body": "AWS Meetup - Magic Night\r\n==========================\r\n\r\nDescription\r\n------------\r\n\r\nThe goal is to use EMR (Spark) to identify earthquake [swarms](https://en.wikipedia.org/wiki/Earthquake_swarm).\r\n\r\nSteps \r\n-----\r\n\r\n1. Launch an EMR cluster with Spark\r\n2. Load earthquake data (provided)\r\n3. Run a clustering algorithm on the data (K-Means algorithm)\r\n4. Visualize the clusters in Google Earth\r\n5. *optional:* Evaluate the precision of the clustering algorithm\r\n6. *mandatory:* Have fun!\r\n\r\nPrerequisites\r\n-------------\r\n\r\n1. Create an AWS account (if you do not have one)\r\n2. Install and configure the AWS command line tool: [link](https://aws.amazon.com/cli/)\r\n3. Install [Google Earth](https://www.google.com/earth/)\r\n\r\nExploring the package\r\n---------------------\r\n1. **create-cluster.sh** - script for launching an EMR cluster (uses the aws cli tool)\r\n2. **earthquakes.csv** - earthquake data (also available at: s3://awsemr-magic-night/earthquakes.csv)\r\n3. **earthquakes.kml** - earthquake data in KML format. You can open this directly with Google Earth. Try it out!\r\n4. **earthquakes_to_kml.py** - script used to convert earthquakes.csv to earthquakes.kml. You won't need this, but you can use it as a resource to learn how to generate a KML file in python.\r\n5. **starter_code.py** - Starter code, let's get hacking!\r\n6. **README.md** - That's me! Hi.\r\n\r\nLaunch an EMR cluster with Spark\r\n--------------------------------\r\n\r\nWe provided you with a helper script to launch an EMR cluster of 1 node. That is all that is necessary for this exercise.\r\n\r\nFirst, log into the AWS console and make sure you have an EC2 keypair. If you do not have one already, create one so that you can `ssh` into the EMR cluster. [Docs](http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html).\r\n\r\nSecondly, let's launch the EMR cluster!\r\n\r\n```\r\n# ./create-cluster.sh [cluster name] [key pair name]\r\n# Example:\r\n./create-cluster.sh aws-magic-night-cluster my-key-pair-name\r\n```\r\n\r\n`ssh` into your EMR cluster. You can find it's public IP in the EC2 Instances listing.\r\n\r\n```\r\n# Example:\r\nssh -i ~/mykey.pem hadoop@ec2-52-24-136-49.us-west-2.compute.amazonaws.com\r\n```\r\n**IMPORTANT**: log in with the `hadoop` user, not `root`. \r\n\r\nIf you are having trouble connecting, ensure that your EC2 security group allows connections on port 22 (for `ssh`).\r\n\r\nGetting Started\r\n---------------\r\n\r\nInstall KML python library, to help you with the generation of KML files from python.\r\n\r\n```\r\nsudo pip install simplekml\r\n```\r\n\r\nStart the Spark python REPL:\r\n\r\n```\r\npyspark\r\n```\r\n\r\nYou are now ready to start tackling the challenge! Good luck!\r\n\r\nStarter Code\r\n------------\r\n\r\nWe provide you with some skeleton code for you to work with. You can copy/paste it into the Spark REPL, and fill in the blanks! Best of luck.\r\n\r\n```\r\nimport csv\r\nfrom pyspark.mllib.clustering import KMeans, KMeansModel\r\nfrom numpy import array\r\nfrom math import sqrt\r\nimport simplekml\r\n\r\n# number of clusters for the K-Means algorithm to detect\r\nnum_clusters = 20\r\n\r\n# use the spark context to load data from S3 (s3://awsemr-magic-night/earthquakes.csv)\r\n...\r\n\r\n# parse out longitude and latitude coordinates from CSV\r\n\r\n# run K-Means Algorithm on data to find the cluster centers\r\n...\r\nmyCenters = ...\r\n\r\n# write cluster data to a KML file for visualization\r\nkml = simplekml.Kml()\r\nfor i, center in enumerate(myCenters):\r\n\r\n    # NOTE: the KML library uses (latitude, longitude) instead of (longitude, latitude)\r\n    pnt = kml.newpoint(name=\"Center #{}\".format(i), coords=[(float(center[1]), float(center[0]))])\r\n    pnt.style.iconstyle.scale = 3\r\n    pnt.style.iconstyle.icon.href = 'http://maps.google.com/mapfiles/kml/shapes/volcano.png'\r\nkml.save(\"clusters.kml\")\r\n\r\n# (OPTIONAL): compute RSS error of K-Means algorithm\r\n\r\n```\r\n\r\nThis is a good [guide](http://spark.apache.org/docs/latest/programming-guide.html) on how to get started on Spark programming. Note that the spark environment automatically declared a `sc` variable, which is the Spark Context. You can use it for loading files from S3.\r\n\r\nPlease use Google for help!\r\n\r\nReferences\r\n----------\r\n\r\n[Spark](http://spark.apache.org/)\r\n\r\n[Spark K-means algorithm](http://spark.apache.org/docs/latest/mllib-clustering.html)\r\n\r\n[K-Means Clustering algorithm](https://en.wikipedia.org/wiki/K-means_clustering)\r\n\r\n[Earthquake data](http://earthquake.usgs.gov/earthquakes/feed/v1.0/csv.php)\r\n\r\n[Generating KML files in python](http://simplekml.readthedocs.io/en/latest/)\r\n\r\nOptional\r\n--------\r\n\r\nCalculate the Residual Sum of Squares error of your clustering model. The [article](http://nlp.stanford.edu/IR-book/html/htmledition/k-means-1.html) may prove useful!",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}